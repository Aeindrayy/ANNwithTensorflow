# -*- coding: utf-8 -*-
"""HOMEWORK2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1sWugVAlwj_SuI4hWooZQDjGwKl1cU6j7
"""

import tensorflow_datasets as tfds
import tensorflow as tf
import matplotlib.pyplot as plt

(train_ds, test_ds), ds_info = tfds.load('mnist', split=['train','test'], as_supervised=True, with_info=True)

tfds . show_examples ( train_ds , ds_info )
# Print shapes,checking pixel vales
sample_data,_ = next(iter(train_ds))

num_train_images = tf.data.experimental.cardinality(train_ds).numpy()#checking number of trained images
num_test_images= tf.data.experimental.cardinality(test_ds).numpy()#checking number of test images

print(sample_data)
print("Number of train images:",num_train_images)
print("Number of test images:",num_test_images)

#pipeline
def prepare_mnist_data(mnist, one_hot_encode=True):
    mnist = mnist.map(lambda img, target: (tf.cast(img, tf.float32), target))
    mnist = mnist.map(lambda img, target: (tf.reshape(img, (-1,)), target))

    if one_hot_encode:
        mnist = mnist.map(lambda img, target: ((img / 128.) - 1., tf.one_hot(target, depth=10)))
    else:
        mnist = mnist.map(lambda img, target: ((img / 128.) - 1., target))

    mnist = mnist.shuffle(500)
    mnist = mnist.batch(30)
    mnist = mnist.prefetch(30)

    return mnist


train_ds= prepare_mnist_data(train_ds)
test_ds= prepare_mnist_data(test_ds)

#deep neural network with Tensorflow+Keras

class ANNModel(tf.keras.Model):
    def __init__(self):
        super(ANNModel, self).__init__()
        self.flatten = tf.keras.layers.Flatten(input_shape=(28 * 28,))
        self.dense1 = tf.keras.layers.Dense(units=256, activation='relu')
        self.dense2 = tf.keras.layers.Dense(units=256, activation='relu')
        self.dense3 = tf.keras.layers.Dense(units=10, activation='softmax')

    def call(self, inputs):
        x = self.flatten(inputs)
        x = self.dense1(x)
        x = self.dense2(x)
        return self.dense3(x)
model=ANNModel()

# Loss function and optimizer
loss_fn = tf.keras.losses.CategoricalCrossentropy()
optimizer = tf.keras.optimizers.SGD(learning_rate=0.1)

# Training loop
epochs = 10
train_losses = []
test_losses = []
train_accuracies = []
test_accuracies = []

for epoch in range(epochs):
    # Training step
    for images, labels in train_ds:
        with tf.GradientTape() as tape:
            predictions = model(images, training=True)
            loss = loss_fn(labels, predictions)
        gradients = tape.gradient(loss, model.trainable_variables)
        optimizer.apply_gradients(zip(gradients, model.trainable_variables))
        train_losses.append(loss.numpy())

    # Evaluation on training set
    train_accuracy, _ = evaluate_model(model, train_ds, loss_fn)
    train_accuracies.append(train_accuracy)

    # Evaluation on test set
    test_accuracy, test_loss = evaluate_model(model, test_ds, loss_fn)
    test_accuracies.append(test_accuracy)
    test_losses.append(test_loss)

    # Print progress
    print(f"Epoch {epoch + 1}/{epochs}, "
          f"Train Loss: {train_losses[-1]}, Train Accuracy: {train_accuracy}, "
          f"Test Loss: {test_loss}, Test Accuracy: {test_accuracy}")

import matplotlib.pyplot as plt

def visualization(train_losses, train_accuracies, test_losses, test_accuracies):
    plt.figure()
    line1, = plt.plot(train_losses, "b-")
    line2, = plt.plot(test_losses, "r-")
    line3, = plt.plot(train_accuracies, "b:")
    line4, = plt.plot(test_accuracies, "r:")
    plt.xlabel("Training steps")
    plt.ylabel("Loss / Accuracy")
    plt.legend((line1, line2, line3, line4), ("Training Loss", "Test Loss", "Train Accuracy", "Test Accuracy"))
    plt.show()




visualization(train_losses, train_accuracies, test_losses, test_accuracies)